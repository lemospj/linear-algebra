<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-linear-subspaces" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Linear subspaces</title>
    <p>
        The notion of a linear subspace of a vector space is a central notion in linear algebra.
    </p>
    <definition xml:id="def-linear-subspace">
        <statement>
            <p>
                A subset <m>V</m> of <m>\R^n</m> is called a <term>linear subspace</term> of <m>\R^n</m> if the following conditions hold:
                <ol>
                    <li>
                        <p>
                            If <m>\bv,\bu\in V</m>, then <m>\bv+\bu\in V</m> (i.e., <m>V</m> is closed under addition).
                        </p>
                    </li>
                    <li>
                        <p>
                            If <m>\lambda\in \R</m> and <m>\bv\in V</m>, then <m>\lambda\bv\in \R</m> (i.e., <m>V</m> is closed under scalar multiplication).
                        </p>
                    </li>
                </ol>
            </p>
        </statement>
    </definition>
    <example>
        <p>
            Consider the set <m>V=\{(x,y)\in\R^2|y=3x\}\subseteq \R^2</m>. This set is a line in the plane containing the origin (the line <m>y=3x</m>), and it is an example of a vector subspace of <m>\R^2</m>. Indeed, if <m>\bv=(x,3x)\in\R^2</m>, then <m>\lambda\bv=(\lambda x, 3\lambda x)</m>, and it is clear that the <m>y</m>-coordinate of <m>\lambda v</m> is three times its <m>x</m>-coordinate, which means that <m>\lambda\bv\in V</m> for every <m>\lambda\in\R</m>. Similarly, if <m>\bv=(x,y)</m> and <m>\bu=(s,t)</m> are both in <m>V</m>, then we must have <m>y=3x</m> and <m>t=3s</m>. This leads to
            <me>
                \bv + \bu = (x,3x)+(s,3s)=(x+s,3(x+s))\in V
            </me>.
            The two axioms of a vector subspace are satisfied, which means that <m>V</m> is a vector subspace of <m>\R^2</m>.
        </p>
    </example>
    <example>
        <p>
            The line <m>y=3x+1</m> is <em>not</em> a vector space. For example, <m>(0,1)</m> is a vector of the form <m>(x, 3x+1)</m>, but <m>2(0,1)=(0,2)</m> is not.
        </p>
    </example>
    <exercise>
        <statement>
            <p>
                Show that if <m>V</m> is a linear subspace of <m>\R^n</m>, then <m>\bz\in V</m>.
            </p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>
                Show that both <m>\{\bz\}</m> and <m>\R^n</m> are linear subspaces of <m>\R^n</m>.
            </p>
        </statement>
    </exercise>
    <exercise>
        <p>
            Show that if <m>V</m> and <m>W</m> are two linear subspaces of <m>\R^n</m>, then <m>V\cap W</m> is a linear subspace of <m>\R^n</m> too.
        </p>
    </exercise>
    <p>
        One interesting example of linear subspaces comes from solution sets of homogeneous linear systems of equations. Indeed, every solution set of a homogeneous linear system of equations is a linear subspace. (See exercises.)
    </p>
    <p>
        If <m>V</m> is a linear subspace of <m>\R^n</m> and <m>S</m> is a subset of vector in <m>V</m>, we say that <m>S</m> <term>generates</term> <m>V</m> if every vector in <m>v</m> can be written as a linear combination of vectors in <m>S</m>.
    </p>
    <definition xml:id="def-span">
        <statement>
            <p>
                Let <m>S</m> be a set of vectors in <m>\R^n</m>. The <term>span</term> of <m>S</m>, denoted <m>\vspan(S)</m>, is the set of vectors in <m>\R^n</m> that can be written as a linear combination of elements in <m>S</m>. We make the convention that the span of the empty set is the linear subspace of <m>\R^n</m> consisting of the zero vector.
            </p>
        </statement>
    </definition>
    <proposition xml:id="prop-span-lin-space">
        <statement>
            <p>
                Let <m>S</m> be a set of vectors in <m>\R^n</m>. The set <m>\vspan(S)</m> is a linear subspace of <m>\R^n</m>.
            </p>
        </statement>
        <proof>
            <p>
                Let <m>\bv</m> be an element in <m>\vspan(S)</m>. First, we will see that <m>\lambda\bv\in\vspan(S)</m> for every scalar <m>\lambda\in \R</m>.
            </p>
            <p>
                By definition, every element <m>v\in \vspan(S)</m> can be written as a linear combination of vectors in <m>S</m>:
                <me>
                    v=\sum_{\bu\in S}c_{\bu}\bu
                </me>,
                where only finitely many of the coefficients <m>c_u</m> are non-zero. If <m>\lambda\in\R</m>, then
                <me>
                    \lambda\bv=\sum_{\bu\in S}\lambda c_{\bu}\bu
                </me>,
                which means that <m>\lambda\bv</m> can also be written as a linear combination of vectors in <m>S</m>. Therefore, <m>\lambda\bv</m> is in the span of <m>S</m>.
            </p>
            <p>
                We now need to show that if <m>\bv</m> and <m>\bw</m> are two vectors in the span of <m>S</m>, then <m>\bv+\bw</m> will also be there.
            </p>
            <p>
                Using the definition of span again, we know that <m>\bv</m> and <m>\bw</m> can be written as linear combinations of vectors in <m>S</m>:
                <me>
                    \bv = \sum_{\bu\in S}a_{\bu}\bu \qquad\text{and}\qquad \bw=\sum_{\bu\in S}b_{\bu}\bu
                </me>.
                Then
                <me>
                    \bv + \bw = \sum_{\bu\in S}(a_{\bu}+b_{\bu})\bu
                </me>, 
                showing that <m>\bv+\bw</m> can also be written as a linear combination of vectors in <m>S</m>. This means that <m>\bv+\bw\in\vspan(S)</m> and concludes the proof that <m>\vspan(S)</m> is a linear subspace of <m>\R^n</m>.
            </p>
        </proof>
    </proposition>
    <example>
        <p>
            Consider the vector <m>\bv=\colvec{1 \\ 2}</m> in <m>\R^2</m>. All linear combinations of the set formed by this single vector are of the form <m>\lambda\bv</m> for some scalar <m>\lambda\in \R</m>. Therefore, <m>\vspan(\bv)</m> is simply the line in <m>\R^2</m> that passes through the origin and has the same direction as the vector <m>\bv</m>.
        </p>
        <figure xml:id="fig-span-line">
            <caption>Vector <m>(1,2)</m> (in blue) and its span (in orange).</caption>
            <image width="80%">
                <shortdescription>The vector minus 1 comma 2, and its span.</shortdescription>
                <prefigure xmlns="https://prefigure.org" label="prefig-span-line">
                    <diagram dimensions="(300,300)" margins="5">
                        <definition>v=(1,2)</definition>
                        <definition>p=(0,0)</definition>
                        <coordinates bbox="(-3,-3, 3, 3)">
                            <grid-axes xlabel="x" ylabel="y" />
                            <line p1="p" p2="v" stroke="orange" infinite="yes" thickness="2"/>
                            <vector v="v" stroke="blue" thickness="2" />
                        </coordinates>
                    </diagram>
                </prefigure>                    
            </image>
        </figure>
        <p>
            Consider now the set <m>S</m> formed by the vectors <m>\bv=\colvec{1 \\ 2}</m> and <m>\bw=\colvec{-2 \\ -4}</m>. Since <m>\bw=-2\bv</m> the span <m>\vspan(\bv,\bw)</m> is the same as <m>\vspan(\bv)</m>. Indeed, if a vector <m>\bu\in\R^2</m> is a linear combination of <m>\bv</m> and <m>\bw</m>, then it is a linear combination of <m>\bv</m> alone:
            <me>
                \bu=\lambda\bv+\mu\bw=\lambda\bv-2\mu\bv=(\lambda-2\mu)\bv
            </me>.
        </p>
        <figure xml:id="fig-span-line2">
            <caption>Vectors <m>(1,2)</m> (in blue) and <m>(-2,-4)</m> (in red) and their span (in orange).</caption>
            <image width="80%">
                <shortdescription>The vectors minus 1 comma 2, negative 2 comma negative 4 and their span.</shortdescription>
                <prefigure xmlns="https://prefigure.org" label="prefig-span-line2">
                    <diagram dimensions="(300,300)" margins="5">
                        <definition>v=(1,2)</definition>
                        <definition>w=(-2,-4)</definition>
                        <definition>p=(0,0)</definition>
                        <coordinates bbox="(-5,-5, 5, 5)">
                            <grid-axes xlabel="x" ylabel="y" />
                            <line p1="p" p2="v" stroke="orange" infinite="yes" thickness="2"/>
                            <vector v="v" stroke="blue" thickness="2" />
                            <vector v="w" stroke="red" thickness="2" />
                        </coordinates>
                    </diagram>
                </prefigure>                    
            </image>
        </figure>
    </example>
    <example xml:id="example-span-two-ind">
        <p>
            Consider the vectors <m>\bv=\colvec{1 \\ 2}</m> and <m>\bu=\colvec{-1 \\ 1}</m>. In this case, <m>\vspan(\bv,\bw)=\R^2</m>. In other words, every vector in <m>\R^2</m> can be written as a linear combination of <m>\bv</m> and <m>\bw</m>. In order to see this, let <m>\bb=\colvec{b_1 \\ b_2}</m> be a vector and <m>\R^2</m>. We want to find real numbers <m>x</m> and <m>y</m> such that
            <me>
                x\colvec{1 \\ 2} + y\colvec{-1 \\ 1}=\colvec{b_1 \\ b_2}
            </me>,
            which leads us to consider the linear system
            <me>
                \left\{
                    \begin{alignedat}{5}
                        x \ampp - \ampp y \amp = \ampp b_1 \\
                        2x \ampp + \ampp y \amp = \ampp b_2
                    \end{alignedat}
                \right.
            </me>
            You may recall that we saw that if the determinant of a system with two equations and two variables is non-zero, then the system has exactly one solution. The determinant of this system is <m>1+2=3</m>, so it always has a single solution.
        </p>
    </example>
    <p>
        In general, how are we to give a nice description of the span of a collection of vectors?
    </p>
    <p>
        Let's say that we have a set of vectors <m>S=\{\bv_1,\ldots,\bv_m\}</m> in <m>\R^n</m>. There are a few operations that we can perform on these vectors that will preserve their span:
        <ol>
            <li>
                <p>
                    When calculating the span, the order in which the generating vectors appear does not matter.
                </p>
            </li>
            <li>
                <p>
                    We may replace any vector in <m>S</m> by a non-zero scalar multiple.
                </p>
            </li>
            <li>
                <p>
                    We may add a multiple of a vector in <m>S</m> to another vector in <m>S</m>.
                </p>
            </li>
        </ol>
    </p>
    <p>
        If you suspect that you are having a d&#233;j&#224; vu, you are right. These properties are analogues of the elementary row operations that we studied in Chapter 1. However, since we represent vectors as columns, we are performing elementary column operations instead!
    </p>
    <p>
        If we now consider the matrix whose columns are the vectors <m>\bv_1,\bv_2,\ldots,\bv_m</m>, every time we perform an elementary column operation on it, each column will still be an element in <m>V=\vspan(\bv_1,\ldots,\bv_m)</m> and the set of all columns will still generate <m>\vspan(\bv_1,\ldots,\bv_m)</m>. We can therefore use the Gauss-Jordan process on columns to end up with a matrix in <term>reduced column echelon form</term> where the columns generate <m>V</m>. Let me show you a few examples that will hopefully convince you that the generating set for <m>\vspan(\bv_1,\ldots,\bv_m)</m> that we obtain from applying the Gauss-Jordan elimination process to the matrix with columns <m>\bv_1,\ldots,\bv_m</m> offers a much better description of this space.
    </p>
    <example>
        <p>
            We start we the two vectors from <xref ref="example-span-two-ind"/>. We want to find a nice description of <m>\vspan(\bv,\bw)</m> where <m>\bv=\colvec{1 \\ 2}</m> and <m>\bw=\colvec{-1 \\ 1}</m>. Let's consider the matrix whose columns are these two vectors:
            <me>
                \begin{pmatrix} 1 \amp -1 \\ 2 \amp 1\end{pmatrix}
            </me>.
            The Gauss-Jordan elimination process <alert>on columns</alert> goes as follows:
            <me>
                \begin{pmatrix} 1 \amp -1 \\ 2 \amp 1\end{pmatrix}\xrightarrow{C_2\to C_2+C_1}\begin{pmatrix} 1\amp 0 \\ 2 \amp 3 \end{pmatrix}\xrightarrow{C_2\to \frac{1}{3}C_2}\begin{pmatrix} 1\amp 0 \\ 2 \amp 1\end{pmatrix}\xrightarrow{C_1\to C_1-2C_2}\begin{pmatrix} 1 \amp 0 \\ 0 \amp 1\end{pmatrix}
            </me>.
            What this means is that <m>\vspan(\bv,\bw)</m> is precisely the set of vectors in <m>\R^2</m> generated by the standard unit vectors <m>\colvec{1 \\ 0}</m> and <m>\colvec{0 \\ 1}</m>. This is of course the whole of <m>\R^2</m>.
        </p>        
    </example>
    <example>
        <p>
            Let's apply the same procedure to understand the span of <m>\bv=\colvec{1 \\ 2}</m> and <m>\bw=\colvec{-2 \\ -4}</m>.
            <me>
                \begin{pmatrix} 1 \amp -2 \\ 2 \amp -4 \end{pmatrix}\xrightarrow{C_2\to C_2+2C_1}\begin{pmatrix} 1 \amp 0 \\ 2 \amp 0\end{pmatrix}
            </me>.
            We conclude that <m>\vspan(\bv,\bw)=\vspan(\bv)</m>.
        </p>
    </example>
    <example>
        <p>
            Consider the vectors <m>\bv_1=\colvec{2 \\ 0 \\ 2},\bv_2 = \colvec{1 \\ 2 \\ 3}</m> and <m>\bv_3=\colvec{0 \\ -1 \\ -1}</m>. Let's try to find a nice description of the span of these three vectors.
            <md>
                <mrow> \begin{pmatrix} 2 \amp 1 \amp 0 \\ 0 \amp 2 \amp -1 \\ 2 \amp 3 \amp -1 \end{pmatrix} \amp \xrightarrow{C_1\to \frac{1}{2}C_1} \begin{pmatrix}1 \amp 1 \amp 0 \\ 0 \amp 2 \amp -1 \\ 1 \amp 3 \amp -1 \end{pmatrix}\xrightarrow{C_2\to C_2-C_1}\begin{pmatrix} 1 \amp 0 \amp 0 \\ 0 \amp 2 \amp -1 \\ 1 \amp 2 \amp -1\end{pmatrix}</mrow>
                <mrow> \amp\xrightarrow{C_2\to\frac{1}{2}C_2} \begin{pmatrix} 1 \amp 0 \amp 0 \\ 0 \amp 1 \amp -1 \\ 1 \amp 1 \amp -1 \end{pmatrix}\xrightarrow{C_3\to C_3-C_2}\begin{pmatrix}1 \amp 0 \amp 0 \\ 0 \amp 1 \amp 0 \\ 1 \amp 1\amp 0\end{pmatrix}</mrow>
            </md>.
            This means that the span of <m>\bv_1,\bv_2,\bv_3</m> is the set of vectors in <m>\R^3</m> of the form
            <me>
                x\colvec{1 \\ 0 \\ 1}+y\colvec{0 \\ 1 \\ 1}=\colvec{x \\ y \\ x+y}
            </me>,
            where <m>x</m> and <m>y</m> are real numbers.
        </p>
    </example>
    <proposition xml:id="prop-finite-basis-span">
        <statement>
            <p>
                Let <m>S</m> be a set of vectors in <m>\R^n</m>. There is a finite subset <m>T\subseteq S</m> that is linearly independent and that satisfies <m>\vspan(T)=\vspan(S)</m>.
            </p>
        </statement>
        <proof>
            <p>
                The key ingredient is the fact that any set of vectors in <m>\R^n</m> with more than <m>n</m> vectors is necessarily linearly dependent. (See <xref ref="exercise-lin-dep"/>)
            </p>
            <p>
                Define <m>V:=\vspan(S)</m>. Start by choosing a maximal subset <m>T</m> of vectors in <m>S</m> that is linearly independent (see the exercises). This means that if we add any other vector of <m>V</m> to <m>T</m> this set becomes linearly dependent. Since <m>T</m> is linearly independent, it contains finitely many vectors. (In fact, at most <m>n</m>.) We only need to show that <m>\vspan(T)=V</m>.
            </p>
            <p>
                Of course, <m>\vspan(T)\subseteq V</m>. If there were a vector <m>\bv\in V</m> not in <m>\vspan(T)</m>, then <m>T\cup\{v\}</m> would be linearly independent. But this cannot happen because <m>T</m> is a maximally linearly independent subset of <m>V</m>. This means that <m>V\subseteq \vspan(T)</m>. Therefore, <m>T</m> spans <m>V</m>.
            </p>
        </proof>
    </proposition>
    <theorem xml:id="thm-span-space">
        <statement>
            <p>
                If <m>V</m> is a linear subspace of <m>\R^n</m>, then there is a finite linearly independent set <m>S</m> of vectors in <m>V</m> such that <m>\vspan(S)=V</m>.
            </p>
        </statement>
        <proof>
        <p>
            This is a direct consequence of Proposition <xref ref="prop-finite-basis-span"/>. Indeed, the set <m>V</m> itself spans <m>V</m>. Using Proposition <xref ref="prop-finite-basis-span"/>, we immediately conclude that there is a finite subset of <m>V</m> that spans <m>V</m> and that is linearly independent.
        </p>
    </proof>
    </theorem>
    <definition xml:id="def-basis">
        <statement>
            <p>
                A linearly independent set <m>S</m> of a linear subspace <m>V</m> of <m>\R^n</m> that spans <m>V</m> is called a <term>basis</term> of <m>V</m>.
            </p>
        </statement>
    </definition>
    <example xml:id="example-standard-basis">
        <p>
            The set of standard unit vectors <m>\be_1,\be_2,\ldots,\be_n</m> in <m>\R^n</m> is a basis of <m>\R^n</m> called the <term>standard basis</term> of <m>\R^n</m>. In order to show this, we need to show two things: first, we need to show that the set of standard unit vectors is linearly independent; after this, we need to show that this set spans <m>\R^n</m>.
        </p>
        <p>
            We already saw that the set of standard of unit vectors is linearly independent, but let's see why this is the case again. Denote by <m>e_{ij}</m> the <m>j</m>th coordinate of <m>\be_i</m>. By definition, we have <m>e_{ij}=0</m> if <m>i\neq j</m> and <m>e_{ii}=1</m>. If <m>\lambda_1,\lambda_2,\ldots,\lambda_n\in\R</m> are such that
            <me>
                \lambda_1\be_1+\lambda_2\be_2+\ldots+\lambda_n\be_n=\bz,
            </me>
            then we have <m>\lambda_1 e_{1j}+\lambda_2 e_{2j}+\ldots+\lambda_n e_{nj}=0</m> for every <m>j</m>. But this means that <m>\lambda_j=0</m> for every <m>j</m>. Therefore, the set of standard unit vectors is linearly independent.
        </p>
        <p>
            Now consider a vector <m>\bv\in\R^n</m>. If <m>x_i</m> denotes the <m>i</m>th entry of <m>\bv</m>, then it is easy to see that <m>\bv=x_1\be_1 +x_2\be_2+\ldots+x_n\be_n</m>, which means that <m>\be_1,\be_2,\ldots,\be_n</m> spans <m>\R^n</m>.
        </p>
    </example>
    <example>
        <p>
            <xref ref="example-standard-basis"/> says, in particular, that the standard basis of <m>\R^2</m> is the set formed by the vectors <m>\colvec{1 \\ 0}</m> and <m>\colvec{0 \\ 1}</m>. But there are many more basis of <m>\R^2</m>. (In fact, there are infinitely many: any two non-collinear vectors are a basis of <m>\R^2</m>.) For example, the vectors <m>\bv=\colvec{-1 \\ 1}</m> and <m>\bu=\colvec{2 \\ 1}</m> are a basis of <m>\R^2</m>. To see this, we first need to show that they are linearly independent, and then that they span <m>\R^2</m>.
        </p>
        <p>
            If <m>a,b\in\R</m> are scalars such that <m>a\bv+b\bu=\bz</m>, then
            <me>
                \colvec{-a \\ a}+\colvec{2b \\ b}=\colvec{0 \\ 0}
            </me>,
            and so <m>2b-a=0</m> and <m>b+a=0</m>. It can be quickly checked that the only solution to this system is <m>a=0</m> and <m>b=0</m>. This means that <m>\bv</m> and <m>\bu</m> are linearly independent.
        </p>
        <p>
            Now we need to show that every vector in <m>\R^2</m> can be expressed as a linear combination of <m>\bv</m> and <m>\bu</m>. Since we know that <m>\colvec{1 \\ 0}</m> and <m>\colvec{0 \\ 1}</m> are a basis of <m>\R^2</m>, it will suffice to show that we can express <m>\colvec{1 \\ 0}</m> and <m>\colvec{0 \\ 1}</m> as linear combinations of <m>\bv</m> and <m>\bu</m>. (Pause here and think why this is true.) Let's show that we can express <m>\colvec{1 \\ 0}</m> as a linear combination of <m>\bv</m> and <m>\bv</m>. The case of <m>\colvec{0 \\ 1}</m> is almost identical.
        </p>
        <p>
            We want to find scalars <m>a,b\in \R</m> such that <m>a\bv+b\bu=\be_1</m>. This means solving the equation
            <me>
                a\colvec{-1 \\ 1} + b\colvec{2 \\ 1}=\colvec{1 \\ 0}
            </me>,
            which is equivalent to the system
            <me>
                \left\{
                    \begin{alignedat}{5}
                        -a \ampp + \ampp 2b \amp = \ampp 1 \\
                        a \ampp + \ampp b \amp = \ampp 0
                    \end{alignedat}
                \right.
            </me>
            Solving the system for <m>a</m> and <m>b</m>, we find that the system has the solution <m>a=-1/3</m> and <m>b=1/3</m>.
        </p>
    </example>
    <p>
        The obvious question now is that of finding a basis for a given vector space. If we are given a finite set of vectors <m>S</m> and the vector space in question in <m>\vspan(S)</m>, then we already know how to do that: this is precisely what we were doing above when we used elementary column operations to find a nice description of <m>\vspan(S)</m>! However, vector spaces may not be described using a given generator set. Instead, they may be given by a set of linear equations.
    </p>
    <example>
        <p>
            Consider the subset <m>V</m> of vectors <m>(x,y,z)</m> of <m>\R^3</m> that satisfy <m>x+2y-z=0</m>. This is a linear subspace of <m>\R^3</m>. This equation has infinitely many solutions that can be given in parametric form by <m>(t-2s, s, t)</m> where <m>s,t\in\R</m>. If <m>s=0</m> and <m>t=1</m>, we obtain the vector <m>\colvec{1 \\ 0 \\ 1}</m>. If we now set <m>s=1</m> and <m>t=0</m>, we get <m>\colvec{-2 \\ 1\\ 0}</m>. It is easy to see that these two vectors are linearly independent and that they span all vectors of the form <m>(t-2s, s, t)</m> because
            <me>
                \colvec{t-2s \\ s \\ t} = t\colvec{1 \\ 0 \\ 1} + s\colvec{-2 \\ 1 \\ 0}
            </me>.
            Therefore, the vectors <m>\colvec{1 \\ 0 \\ 1}</m> and <m>\colvec{-2 \\ 1 \\0}</m> form a basis of <m>V</m>.
        </p>
    </example>
    <example>
        <p>
            Let <m>V</m> be the linear subspace of <m>\R^4</m> consisting of vectors <m>(x,y,z,w)</m> in <m>\R^4</m> satisfying the equations <m>2x-3z=0</m> and <m>2y+z+w=0</m>. Solving the system, we see that <m>z</m> and <m>w</m> are free variables and the solutions to the system can be given in parametric form as <m>\left(\frac{3}{2}s, -\frac{1}{2}s-\frac{1}{2}t, s, t\right)</m>, where <m>s,t\in\R</m>. Setting <m>s=0</m> and <m>t=2</m>, we find the solution <m>\colvec{0 \\ -1 \\ 0 \\ 2}</m>. If we now set <m>s=2</m> and <m>t=0</m>, we get <m>\colvec{3 \\ -1 \\ 2 \\ 0}</m>. Since these two vectors are linearly independent and
            <me>
                \colvec{\frac{3}{2}s \\ -\frac{1}{2}s-\frac{1}{2}t \\ s \\ t} = \frac{s}{2}\colvec{3 \\ -1 \\ 2 \\ 0}+\frac{t}{2}\colvec{0 \\ -1 \\ 0 \\ 2}
            </me>,
            they constitute a basis of <m>V</m>.
        </p>
    </example>
    <p>
        So every linear subspace of <m>\R^n</m> has a basis. While there are infinitely many basis for a given non-zero linear subspace of <m>\R^n</m>, they will all have the same size.
    </p>
    <lemma xml:id="lem-more-vectors-than-dimension">
        <statement>
            <p>
                Let <m>V</m> be a linear subspace of <m>\R^n</m>. If <m>B</m> is a basis of <m>V</m> and <m>S</m> is a subset of vectors of <m>V</m> whose size is strictly larger than the size of <m>B</m>, then <m>S</m> is linearly dependent.
            </p>
        </statement>
        <proof>
            <p>
                Let <m>n</m> be the size of <m>B</m> and <m>m</m> be the size of <m>S</m>. By hypothesis, <m>m\gt n</m>. Let <m>B=\{\bv_1,\ldots,\bv_n\}</m> and <m>S=\{\bu_1,\ldots,\bu_m\}</m>. Since <m>B</m> is a basis, every vector in <m>S</m> can be written as a linear combination of vectors in <m>B</m>. This means that for each <m>1\leq j\leq m</m>, there are scalars <m>a_{1j}, a_{2j},\ldots, a_{nj}</m> such that 
                <me>
                    a_{1j}\bv_1+a_{2j}\bv_2+\ldots + a_{nj}\bv_n=\bu_j
                </me> for every <m>j</m> in <m>\{1,\ldots,m\}</m>. The aim is to use this to find a non-trivial linear combination of the <m>\bu_i</m>'s that represents the zero vector.
            </p>
            <p>
                To be clear, we are trying to find scalars <m>x_1,x_2,\ldots,x_m</m>, not all zero, such that <m>x_1\bu_1+\x_2\bu_2+\ldots+x_n\bu_n=\bz</m>. It will be enough that these scalars satisfy
                <me>
                    x_1a_{i1} + x_2a_{i2}+\ldots+x_ma_{im}=0
                </me>. 
                (You should try to figure out why this is true.) In other words, we need to solve a homogeneous linear system with <m>n</m> equations and <m>m</m> variables. Since <m>m\gt n</m>, there are infinitely many solutions, which means that there are non-zero solutions too.
            </p>
        </proof>
    </lemma>
    <theorem xml:id="thm-dimension">
        <statement>
            <p>
                Let <m>V</m> be a linear subspace of <m>\R^n</m>. Every basis of <m>V</m> has the same size.
            </p>
        </statement>
        <proof>
            <p>
                Suppose, for the sake of contradiction, that we have two basis <m>B_1</m> and <m>B_2</m> of different sizes. Say that the size of <m>B_1</m> is <m>n</m> and that of <m>B_2</m> is <m>m</m>. We may assume, without loss of generality, that <m>m\gt n</m>. In that case, <xref ref="lem-more-vectors-than-dimension"/> implies that <m>B_2</m> is linearly dependent, which contradicts the fact that it is a basis. Therefore, <m>B_1</m> and <m>B_2</m> must have the same size.
            </p>
        </proof>
    </theorem>
    <definition xml:id="def-dimension">
        <statement>
            <p>
                Let <m>V</m> be a linear subspace of <m>\R^n</m>. The size of any basis of <m>V</m> is called the <term>dimension</term> of <m>V</m>.
            </p>
        </statement>
    </definition>
    <p>
        This definition allows us to restate <xref ref="lem-more-vectors-than-dimension"/>. Indeed, we may simply say that any subset of <m>V</m> of size <m>\gt \dim(V)</m> is linearly dependent.
    </p>
    <example>
        <p>
            Consider the linear subspace <m>V</m> of <m>\R^4</m> defined as the set of solutions to the equation <m>x-2y+3z-w=0</m>. The variables <m>y,z</m> and <m>w</m> are free variables. The set of solutions to this equation in <m>\R^4</m> can be given in parametric form by <m>(2s-3t+r,s,t,r)</m>, where <m>s,t,r\in\R</m>. It is easy to see that the vectors <m>\bv = \colvec{2 \\ 1 \\ 0 \\ 0}</m>, <m>\bu=\colvec{-3 \\ 0 \\ 1 \\ 0}</m> and <m>\bw = \colvec{1 \\ 0 \\ 0 \\ 1}</m> are linearly independent and that
            <me>
                \colvec{2s-3t+r \\ s \\ t \\ r} = s\bv + t\bu+r\bw
            </me>
            for any <m>s,t,u\in\R</m>. This means that <m>\bv, \bu</m> and <m>\bw</m> form a basis of <m>V</m>. Therefore, <m>\dim(V)=3</m>. 
        </p>
    </example>
    <p>
        Note that the number of free variables in a linear system always matches the dimension of the solution set of that linear system.
    </p>
    <p>
        If one linear subspace is contained in another, then, as you might expect, the dimension of the one that contains is at least as big as that of the one that is contained. This is the content of the following proposition. Try to prove it using <xref ref="lem-more-vectors-than-dimension"/>.
    </p>
    <proposition xml:id="prop-dimension-containment">
        <statement>
            <p>
                Let <m>V</m> and <m>W</m> be two linear subspaces of <m>\R^n</m>. If <m>V\subseteq W</m>, then <m>\dim(V)\leq \dim(W)</m>.
            </p>
        </statement>
        <proof>
            <p>
                Suppose, for the sake of contradiction, that there are linear subspaces <m>V</m> and <m>W</m> of <m>\R^n</m> such that <m>V\subseteq W</m> and <m>\dim(V)\gt \dim(W)</m>. Let <m>B</m> be a basis of <m>V</m>. Since <m>V</m> is contained in <m>W</m>, the set <m>B</m> is contained in <m>W</m> as well. However, <m>\#B=\dim(V)\gt \dim(W)</m>, which, according to <xref ref="lem-more-vectors-than-dimension"/>, means that <m>B</m> is linearly dependent. This, however, contradicts the fact that it is a basis of <m>V</m>. It follows that we cannot have <m>\dim(V) \gt \dim(W)</m>, and so it must be the case that <m>\dim(V)\leq \dim(W)</m>.
            </p>
        </proof>
    </proposition>
    <p>
        We can actually say a little bit more about this. We will now see that if <m>V\subseteq W</m> are two linear subspaces of <m>\R^n</m> such that <m>\dim(V)=\dim(W)</m>, then <m>V=W</m>.
    </p>
    <proposition xml:id="prop-lin-ind-dimension">
        <statement>
            <p>
                Let <m>V</m> be a linear subspace of <m>\R^n</m> of dimension <m>d</m>. Any linearly independent set of vectors in <m>V</m> of size <m>d</m> is a basis of <m>V</m>.
            </p>
        </statement>
        <proof>
            <p>
                Let <m>S</m> be a subset of vectors of <m>V</m> of size <m>d</m> that is linearly independent. We only need to show that every vector of <m>V</m> can be written as a linear combination of vectors in <m>S</m>.
            </p>
            <p>
                Choose a vector <m>v\in V</m>. We know from <xref ref="lem-more-vectors-than-dimension"/> that any set of vectors in <m>V</m> whose size is larger than <m>d</m> is linearly dependent. This means that <m>S\cup\{d\}</m> must be linearly dependent because its size is <m>d+1</m>. Using <xref ref="lem-generate"/>, we conclude that the vector <m>v</m> can be written as a linear combination of vectors in <m>S</m>. This shows that <m>S</m> is a basis of <m>V</m>.
            </p>
        </proof>
    </proposition>
    <p>
        The following is an immediate consequence of this result. You should try to prove it!
    </p>
    <corollary xml:id="cor-containment-spaces">
        <statement>
            <p>
                Let <m>V</m> and <m>W</m> be linear subspaces of <m>\R^n</m> such that <m>V\subseteq W</m>. If <m>\dim(V)=\dim(W)</m>, then <m>V=W</m>.
            </p>
        </statement>
    </corollary>
</section>