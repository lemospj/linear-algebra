<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-linear-dependence" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Linear dependence</title>
    <definition xml:id="def-linear-combination">
        <statement>
            <p>
                Let <m>\bv_1,\bv_2,\ldots,\bv_m</m> be vectors in <m>\R^n</m>. A <term>linear combination</term> of <m>\bv_1,\bv_2,\ldots,\bv_m</m> is an expression of the form
                <me>
                    \lambda_1\bv_1+\lambda_2\bv_2+\ldots+\lambda_m\bv_m
                </me>,
                where <m>\lambda_1,\lambda_2,\ldots,\lambda_m</m> are real numbers. The scalars <m>\lambda_1,\lambda_2,\ldots,\lambda_m</m> are called the <term>coefficients</term> of the linear combination.
            </p>
        </statement>
    </definition>
    <p>
        The linear combination is the <em>expression</em> itself. Two different linear combinations can give rise to the same vector.
    </p>
    <example xml:id="ex-lin-comb77">
        <p>
            The expression
            <me>
                2\colvec{1 \\ -1} + 3\colvec{2 \\ 3} - \colvec{1 \\ 0}
            </me>
            is a linear combination of the vectors <m>\colvec{1 \\ -1}</m>, <m>\colvec{2 \\ 3}</m> and <m>\colvec{-1 \\ 0}</m> in <m>\R^2</m>. The resulting vector is 
            <me>
               2\colvec{1 \\ -1} + 3\colvec{2 \\ 3}-\colvec{1 \\ 0}=\colvec{2 \\ -2} + \colvec{6 \\ 9}-\colvec{1 \\ 0}=\colvec{7 \\ 7}
            </me>.
            Here is a different linear combination of the same vectors that also gives rise to <m>\colvec{7 \\ 7}</m>:
            <me>
                -\colvec{1 \\ -1} + 2\colvec{2 \\ 3} + 4\colvec{1 \\ 0} = \colvec{-1 \\ 1} + \colvec{4 \\ 6} + \colvec{4 \\ 0}=\colvec{7 \\ 7}
            </me>.
        </p>
    </example>
    <example xml:id="ex-lin-comb77-unique">
        <p>
            In <xref ref="ex-lin-comb77"/> we saw that it is possible to express <m>\colvec{7 \\ 7}</m> in more than one way as linear combination of the vectors <m>\colvec{1 \\ -1}, \colvec{2 \\ 3}</m> and <m>\colvec{1 \\ 0}</m>. However, there is only one linear combination of the vectors <m>\be_1=\colvec{1 \\ 0}</m> and <m>\be_2=\colvec{0 \\ 1}</m> that results in <m>\colvec{7 \\ 7}</m>. Indeed, the problem is that of finding real numbers <m>\lambda_1</m> and <m>\lambda_2</m> such that
            <me>
                \lambda_1\be_1+\lambda_2\be_2=\colvec{7 \\ 7}
            </me>.
            However, 
            <me>
                \lambda_1\be_1+\lambda_2\be_2= \lambda_1\colvec{1 \\ 0}+\lambda_2\colvec{0 \\ 1}= \colvec{\lambda_1 \\ \lambda_2}
            </me>,
            which means that we must have <m>\lambda_1=\lambda_2=7</m>.
        </p>
    </example>
    <example>
        <p>
            Sometimes it is not possible to represent a vector as a linear combination of prescribed vectors. For example, the vector
            <me>
                \bv=\colvec{3 \\ 2 \\ 1}
            </me>
            in <m>\R^3</m> cannot be represented as a linear combination of the vectors
            <me>
                \bw=\colvec{2 \\ 1 \\ 0}\quad \text{and}\quad \bu=\colvec{1 \\ 5 \\ 0}
            </me>
            because any linear combination of the vectors <m>\bw</m> and <m>\bu</m> will have <m>0</m> as its third coordinate.
        </p>
    </example>
    <p>
        <xref ref="ex-lin-comb77-unique"/> shows that sometimes there is at most one way to represent a vector as a linear combination of prescribed vectors. This phenomenon occurs when there are no linear relationships among the vectors that are to be combined.
    </p>
    <definition xml:id="def-linear-independence">
        <statement>
            <p>
                We say that the vectors <m>\bv_1,\bv_2,\ldots,\bv_m\in\R^n</m> are said to be <term>linearly dependent</term> if there exist real numbers <m>\lambda_1,\lambda_2,\ldots,\lambda_m</m>, <em>not all zero</em>, such that
                <me>
                    \lambda_1\bv_1+\lambda_2\bv_2+\ldots+\lambda_m\bv_m=\bz
                </me>.
                If the vectors <m>\bv_1,\bv_2,\ldots,\bv_m</m> are not linearly dependent, then we say that they are <term>linearly independent</term>.
            </p>
        </statement>
    </definition>
    <p>
        The condition that the coefficients of the linear combination above are not all zero in the definition of linear dependence is crucial. Indeed, for any given set of vectors <m>\bv_1,\bv_2,\ldots,\bv_m</m>, it is always possible to find a trivial linear combination that represents the zero vector. All we need to do is to choose every coefficient to be <m>0</m>:
        <me>
            0\cdot\bv_1+0\cdot\bv_2+\ldots+0\cdot\bv_m=\bz
        </me>.
        The vectors  <m>\bv_1,\bv_2,\ldots,\bv_m</m> are only linearly dependent if there exists a linear combination of these vectors representing the zero vector <em>other</em> than the trivial one. In particular, if you wish to prove that a given set of vectors <m>\bv_1,\bv_2,\ldots,\bv_n</m> is linearly independent, all you need to do is to show that if <m>\lambda_1,\lambda_2,\ldots,\lambda_n</m> are real numbers such that
        <me>
            \lambda_1\bv_1+\lambda_2\bv_2+\ldots+\lambda_n\bv_n=\bz,
        </me>
        then all the <m>\lambda_i</m> equal <m>0</m>.
    </p>
    <example>
        <p>
            If <m>\bv\in\R^n</m> is non-zero, then the set consisting only of <m>\bv</m> is linearly independent. Indeed, if <m>\lambda\bv=0</m>, then we must have <m>\lambda=0</m> because, by assumption, <m>\bv\neq \bz</m>.
        </p>
    </example>
    <example>
        <p>
            The vectors <m>\colvec{1 \\ 2}</m> and <m>\colvec{2 \\ 4}</m> are linearly dependent. Indeed,
            <me>
                2\colvec{1 \\ 2}-\colvec{2 \\ 4}=\bz
            </me>.
        </p>
        <figure xml:id="fig-lin-dep">
            <caption>The standard unit vectors in <m>\R^2</m>.</caption>
                <image width="80%">
                    <shortdescription>The standard unit vectors of r two.</shortdescription>
                    <prefigure xmlns="https://prefigure.org" label="prefig-lin-dep">
                        <diagram dimensions="(300,300)" margins="5">
                            <definition>v=(1,2)</definition>
                            <definition>w=(2,4)</definition>
                            <coordinates bbox="(-1.5,-1.5, 4.5, 4.5)">
                                <grid-axes xlabel="x" ylabel="y" />
                                <vector v="w" stroke="orange" thickness="3" />
                                <vector v="v" stroke="blue" thickness="3" opacity="0.5"/>
                            </coordinates>
                        </diagram>
                    </prefigure>                    
                </image>
        </figure>
    </example>
    <example xml:id="example-standard-units">
        <p>
            The set formed by the vectors <m>\be_1:=\colvec{1 \\ 0}</m> and <m>\be_2:=\colvec{0 \\ 1}</m> is linearly independent. Indeed, say that <m>\lambda_1,\lambda_2\in\R</m> are real numbers satisfying
            <me>
                \lambda_1\be_1+\lambda_2\be_2=\bz
            </me>.
            Let's start by evaluating the left-hand side of the equation above:
            <me>
                \lambda_1\be_1+\lambda_2\be_2=\lambda_1\colvec{1 \\ 0}+\lambda_2\colvec{0 \\ 1}=\colvec{\lambda_1 \\ \lambda_2}
            </me>.
            So we have
            <me>
                \colvec{\lambda_1 \\ \lambda_2}= \colvec{0 \\ 0}
            </me>,
            which means that <m>\lambda_1=0</m> and <m>\lambda_2=0</m>. This shows that <m>\be_1</m> and <m>\be_2</m> are linearly independent.
        </p>
        <figure xml:id="fig-standard-unit">
            <caption>The standard unit vectors in <m>\R^2</m>.</caption>
                <image width="80%">
                    <shortdescription>The standard unit vectors of r two.</shortdescription>
                    <prefigure xmlns="https://prefigure.org" label="prefig-standard-unit">
                        <diagram dimensions="(300,300)" margins="5">
                            <definition>e1=(1,0)</definition>
                            <definition>e2=(0,1)</definition>
                            <coordinates bbox="(-1.5,-1.5, 1.5, 1.5)">
                                <grid-axes xlabel="x" ylabel="y" />
                                <vector v="e1" stroke="orange" thickness="3" />
                                <vector v="e2" stroke="#7497F1" thickness="3" />
                            </coordinates>
                        </diagram>
                    </prefigure>                    
                </image>
        </figure>
    </example>
    <example>
        <p>
            The vectors <m>\colvec{1 \\ 2 \\ 3}</m> and <m>\colvec{2 \\ 1 \\ 3}</m> are linearly independent. To see this, let <m>\lambda</m> and <m>\mu</m> be real scalars such that
            <me>
                \lambda\colvec{1 \\ 2 \\ 3} + \mu\colvec{2 \\ 1 \\ 3} = \colvec{0 \\ 0 \\ 0}
            </me>.
            Then
            <me>
                \colvec{\lambda + 2\mu \\ 2\lambda + \mu \\ 3\lambda + 3\mu} = \colvec{0 \\ 0 \\ 0}
            </me>.
            In other words, we need to solve the system
            <me>
                \left\{
                    \begin{alignedat}{5}
                        \lambda \ampp + \ampp 2\mu  \amp = \ampp 0 \\
                        2\lambda \ampp + \ampp \mu  \amp = \ampp 0 \\
                        3\lambda \ampp + \ampp 3\mu \amp = \ampp 0
                    \end{alignedat}
                \right.
            </me>
            in the variables <m>\lambda</m> and <m>\mu</m>. But it is easy to see that the only solution to this system is given by <m>\lambda=0</m> and <m>\mu=0</m>.
        </p>
    </example>
    <p>
        Let me mention an important generalization of <xref ref="example-standard-units"/>. Recall that the standard unit vectors in <m>\R^n</m> are the vectors
        <me>
            \be_1=\colvec{1 \\ 0 \\ 0 \\ \vdots \\0},\qquad\be_2=\colvec{0 \\ 1 \\ 0 \\ \vdots \\0},\qquad\cdots,\qquad\be_n=\colvec{0 \\ 0 \\ 0 \\ \vdots \\1}
        </me>.
    </p>
    <lemma xml:id="lem-standard-ind">
        <statement>
            <p>
                The set of standard unit vectors in <m>\R^n</m> is linearly independent.
            </p>
        </statement>
        <proof>
            <p>
                Let <m>\lambda_1,\ldots,\lambda_n\in\R</m> be real numbers. If we denote by <m>e_{ij}</m> the <m>j</m>th entry of <m>\be_i</m>, then the <m>j</m>th entry of <m>\lambda_1\be_1+\ldots+\lambda_n\be_n</m> is <m>\lambda_1e_{1j}+\lambda_2e_{2j}+\ldots+\lambda_ne_{nj}</m>. By the definition of standard unit vectors, we have <m>e_{ij}=0</m> whenever <m>i\neq j</m>, and <m>e_{ii}=1</m> for every <m>i</m>. Therefore, the <m>j</m>th entry of <m>\lambda_1\be_1+\ldots+\lambda_n\be_n</m> is
                <me>
                    \lambda_1e_{1j}+\lambda_2e_{2j}+\ldots+\lambda_ne_{nj}=\lambda_j
                </me>.
                If we suppose that the <m>\lambda_i</m> are such that
                <me>
                    \lambda_1\be_1 + \lambda_2\be_2 + \ldots + \lambda_n \be_n=\bz
                </me>,
                we conclude that <m>\lambda_j=0</m> for every <m>1\leq j\leq n</m>.
            </p>
        </proof>
    </lemma>
    <lemma xml:id="lem-unique-lin-comb">
        <statement>
            <p>
                Let <m>\bw</m> be a vector in <m>\R^n</m>, and let <m>\bv_1,\ldots,\bv_m\in\R^n</m> be a set of <m>m</m> linearly independent vectors in <m>\R^n</m>. Then there is at most one linear combination <m>\lambda_1\bv_1+\ldots+\lambda_m\bv_m</m> of these <m>m</m> linearly independent vectors such that
                <me>
                    \bw=\lambda_1\bv_1+\ldots+\lambda_m\bv_m
                </me>.
            </p>
        </statement>
        <proof>
            <p>
                Note that there may well be no linear combination of the linearly independent vectors that represents <m>\bw</m>. The claim is that if there is one, that is the only one. To prove this, suppose that we have two linear combinations of <m>\bv_1,\ldots,\bv_m</m> representing <m>\bw</m>:
                <me>
                    \lambda_1\bv_1+\lambda_2\bv_2+\ldots+\lambda_m\bv_m=\bw
                </me>
                and
                <me>
                    \mu_1\bv_1+\mu_2\bv_2+\ldots+\mu_m\bv_m=\bw
                </me>,
                where <m>\lambda_1,\lambda_2,\ldots,\lambda_m,\mu_1,\mu_2,\ldots,\mu_m\in\R</m>. If we subtract the second linear combination from the first we end up with
                <me>
                    (\lambda_1-\mu_1)\bv_1+(\lambda_2-\mu_2)\bv_2+\ldots+(\lambda_m-\mu_m)\bv_m=\bw-\bw=\bz
                </me>.
                Since the set <m>\bv_1,\ldots,\bv_m</m> is linearly independent, we must have <m>\lambda_i-\mu_i=0</m> for all <m>i</m>. Put differently, <m>\lambda_i=\mu_i</m> for all <m>i</m>. This means that the two linear combinations are actually the same.
            </p>
        </proof>
    </lemma>
    <p>
        To be absolutely clear, <xref ref="lem-unique-lin-comb"/> <em>does not</em> say that you will be capable of representing any vector in <m>\R^n</m> using linear combinations of a given linearly independent set of vectors. For example, the set consisting of the vector <m>\be_1=\colvec{1 \\ 0}\in\R^2</m> alone is linearly independent. Linear combinations involving only <m>\be_1</m> are of the form <m>\lambda\be_1</m>, where <m>\lambda\in\R</m>. This means that the only vectors in <m>\R^2</m> that can be represented as linear combinations of <m>\be_1</m> are of the form <m>\colvec{\lambda \\ 0}</m>. This leaves out vectors like <m>\be_2</m>.
    </p>
    <p>
        What <xref ref="lem-unique-lin-comb"/> does say is that if it is possible to represent a vector using a linear combination of linearly independent vectors, then there is only one way of doing so.
    </p>
    <p>
        Linearly independent sets of vectors of <m>\R^n</m> that are capable of representing any other vector in <m>\R^n</m> are very special and very important in linear algebra. We will discuss these sets in a later section.
    </p>
    <p>
        Before finishing this section, I will state one more useful result. As with every result in these lecture notes, you should attempt to convince yourself of the truthfulness of the statement before looking at the proof. Sometimes you may be able to prove the results yourself, and you will learn a great deal by doing so. But at the very least, work out a couple of examples. The next lemma, in particular, is relatively easy to prove, so you could regard it as an exercise.
    </p>
    <lemma xml:id="lem-lin-dep-comb">
        <statement>
            <p>
                Let <m>m\geq 2</m> be an integer. If <m>\bv_1,\bv_2,\ldots,\bv_m\in\R^n</m> is a linearly dependent set of vectors, then at least one of these vectors can be written as a linear combination of the other vectors in the set.
            </p>
        </statement>
        <proof>
            <p>
                If one of the vectors is the zero vector, then the result is trivial. Indeed, the zero vector can always be expressed as the trivial linear combination (i.e. coefficients are all <m>0</m>) of any finite set of vectors. We are therefore reduced to the case where all the vectors are non-zero.
            </p>
            <p>
                Suppose that all the vectors <m>\bv_1,\bv_2,\ldots,\bv_m</m> are non-zero. Since they are linearly dependent, there is a set of <m>m</m> scalars <m>\lambda_1,\lambda_2,\ldots,\lambda_m</m>, not all zero, such that
                <men xml:id="equation-lin-comb">
                    \lambda_1\bv_1+\lambda_2\bv_2+\ldots+\lambda_m\bv_m=\bz
                </men>.
                We may assume, without loss of generality, that <m>\lambda_1\neq 0</m>, because if this is not the case we can just reorder the vectors in the set and the scalars. (In other words, the labels that we assigned to the vectors are completely arbitrary and irrelevant.) But then, Equation <xref ref="equation-lin-comb"/> is equivalent to
                <me>
                    \lambda_2\bv_2+\ldots+\lambda_m\bv_m=-\lambda_1\bv_1
                </me>.
                Since <m>\lambda_1\neq 0</m>, we may divide both sides by <m>-\lambda_1</m>, from where we get
                <me>
                    \bv_1=-\frac{\lambda_2}{\lambda_1}\bv_2-\ldots-\frac{\lambda_m}{\lambda_1}\bv_m
                </me>.
                In other words, we wrote <m>\bv_1</m> as a linear combination of the other vectors in the set.
            </p>
        </proof>
    </lemma>
    <lemma xml:id="lem-generate">
        <statement>
            <p>
                Let <m>S</m> be a linearly independent set of vectors in <m>\R^n</m>. If <m>\bv\in\R^n</m> is a vector such that <m>S\cup\{\bv\}</m> is linearly dependent, then <m>\bv</m> can be written as a linear combination of vectors in <m>S</m>.
            </p>
        </statement>
        <proof>
            <p>
                Since <m>S\cup\{v\}</m> is linearly dependent, there is a collection of vectors <m>\bu_1,\bu_2,\ldots,\bu_m\in S</m> and scalars <m>a_1,a_2,\ldots,a_m,c\in\R</m> not all <m>0</m> such that
                <me>
                    a_1\bu_1+a_2\bu_2+\ldots+a_m\bu_m+c\bv=\bz
                </me>.
                Note that we must have <m>c\neq 0</m>, for otherwise we would have a non-trivial linear relation among vectors in <m>S</m>, meaning that <m>S</m> itself would be linearly dependent. But if <m>c\neq 0</m>, we have
                <me>
                    \bv = -\frac{a_1}{c}\bu_1 -\frac{a_2}{c}\bu_2-\ldots-\frac{a_m}{c}\bu_m
                </me>, as we wanted.
            </p>
        </proof>
    </lemma>
    <exercises xml:id="exercises-chapter2-section2">
        <exercise>
            <statement>
                <p>
                    For each subquestion, determine whether the given set of vectors is linearly independent. If not, find a non-trivial linear combination representing the zero vector.
                </p>
                <ol marker="(a)">
                    <li>
                        <p>
                            <m>\colvec{1 \\ 2},\colvec{-1 \\ -5}</m>
                        </p>
                    </li>
                    <li>
                        <p>
                            <m>\colvec{0 \\ 3},\colvec{-2 \\ 3},\colvec{1 \\ 0}</m>
                        </p>
                    </li>
                    <li>
                        <p>
                            <m>\colvec{1 \\ 2 \\ 0},\colvec{3 \\ 0 \\ -1},\colvec{1 \\ 1 \\ 1}</m>
                        </p>
                    </li>
                    <li>
                        <p>
                            <m>\colvec{1 \\ 0 \\ -1},\colvec{2 \\ 1 \\ 0}, \colvec{3 \\ 1 \\ -1}</m>
                        </p>
                    </li>
                </ol>
            </statement>
        </exercise>
        <exercise>
            <p>
                In each subquestion, you will be given a vector <m>\bw</m> and a list of vectors <m>\bv_1, \bv_2,\ldots</m>. Determine whether it is possible to write the vector <m>\bw</m> as a linear combination of the vectors <m>\bv_i</m>. If it is, find one such linear combination.
            </p>
            <ol marker="(a)">
                <li>
                    <p>
                        <m>\bw=\colvec{1 \\ 0}, \quad \bv_1=\colvec{2 \\ 1}, \bv_2=\colvec{3 \\ 1}</m>
                    </p>
                </li>
                <li>
                    <p>
                        <m>\bw=\colvec{0 \\ 1},\quad \bv_1=\colvec{2 \\ 1}, \bv_2=\colvec{3 \\ 1}</m>
                    </p>
                </li>
                <li>
                    <p>
                        <m>\bw=\colvec{1 \\ 0},\quad \bv_1=\colvec{2 \\ 1}, \bv_2=\colvec{-4 \\ -2}</m>
                    </p>
                </li>
                <li>
                    <p>
                        <m>\bw = \colvec{1 \\ 2 \\ 0},\quad \bv_1=\colvec{1 \\ -1 \\ 3},\bv_2=\colvec{0 \\ 1 \\ -1}</m>
                    </p>
                </li>
                <li>
                    <p>
                        <m>\bw=\colvec{1 \\ 2 \\ 0},\quad \bv_1=\colvec{1 \\ -1 \\ 3},\bv_2=\colvec{-3 \\ 1 \\ 0}</m>
                    </p>
                </li>
                <li>
                    <p>
                        <m>\bw=\colvec{1 \\ 1 \\ 1},\quad \bv_1=\colvec{1 \\ -1 \\ 0},\bv_2=\colvec{-1 \\ 2 \\ 0}, \bv_3=\colvec{0 \\ 2 \\ -1}</m>
                    </p>
                </li>
            </ol>
        </exercise>
        <exercise>
            <statement>
                <p>
                    Consider the vectors <m>\bv=\colvec{3 \\ 1 \\ -1}</m> and <m>\bw=\colvec{1 \\ 2 \\ 1}</m> in <m>\R^3</m>.
                </p>
                <ol marker="(a)">
                    <li>
                        <p>
                            Show that the vectors <m>\bv</m> and <m>\bw</m> are linearly independent.
                        </p>
                    </li>
                    <li>
                        <p>
                            Find a vector <m>\bu_1\in\R^3</m> such that the set formed by the vectors <m>\bv,\bw</m> and <m>\bu_1</m> is still linearly independent.
                        </p>
                    </li>
                    <li>
                        <p>
                            Find a vector <m>\bu_2\in\R^3</m> such that the set formed by the vectors <m>\bv,\bw</m> and <m>\bu_2</m> is linearly dependent.
                        </p>
                    </li>
                </ol>
            </statement>
        </exercise>
        <exercise>
            <statement>
                <p>
                    Suppose that <m>S=\{\bv_1,\bv_2,\ldots,\bv_m\}\subseteq \R^n</m> is a linearly independent set of vectors. Is it possible to find a subset of <m>S</m> that is linearly dependent?
                </p>
            </statement>
        </exercise>
        <exercise>
            <statement>
                <p>
                    Show that two vectors in <m>\R^n</m> are linearly independent if and only if one of them is a scalar multiple of the other.
                </p>
            </statement>
        </exercise>
        <exercise>
            <statement>
                <p>
                    Find an example of a linearly independent set in <m>\R^2</m> consisting of more than one vector such that none of them is a scalar multiple of another vector in the set.
                </p>
            </statement>
        </exercise>
        <exercise xml:id="exercise-ortho-lin-ind">
            <statement>
                <p>
                    &#9733; Show that if <m>\bv,\bw\in\R^n</m> are two non-zero vectors such that <m>\bv\cdot\bw=0</m>, then they are linearly independent.
                </p>
            </statement>
            <hint>
                <p>
                    Use the properties of the dot product to simplify <m>(\lambda\bv + \mu\bw)\cdot\bv</m> and <m>(\lambda\bv+\mu\bw)\cdot\bw</m>, where <m>\lambda,\mu\in\R</m> are scalars.
                </p>
            </hint>
        </exercise>
        <exercise>
            <statement>
                <p>
                    The converse of <xref ref="exercise-ortho-lin-ind"/> does not hold. Give an example of two linearly independent vectors in <m>\R^2</m> such that <m>\bv\cdot \bw\neq 0</m>.
                </p>
            </statement>
        </exercise>
        <exercise xml:id="exercise-lin-dep">
            <statement>
                <p>
                    &#9733; Let <m>m,n</m> be positive integers such that <m>m &gt; n</m>. Show that any set of <m>m</m> vectors <m>\bv_1,\bv_2,\ldots,\bv_m</m> in <m>\R^n</m> is linearly dependent.
                </p>
            </statement>
            <hint>
                <p>
                    You may want to use the fact that any homogeneous linear system with more variables than equations has infinitely many solutions.
                </p>
            </hint>
        </exercise>
        <exercise>
            <statement>
                <p>
                    Let <m>A</m> be an <m>n\times n</m> matrix.
                    <ol marker="(a)">
                        <li>
                            <p>
                                Show that if one of the columns of <m>A</m> is <m>\bz</m>, then <m>\det(A)=0</m>.
                            </p>
                        </li>
                        <li>
                            <p>
                                &#9733; Show that if the columns of <m>A</m> form a linear dependent set of vectors, then <m>\det(A)=0</m>.
                            </p>
                        </li>
                    </ol>
                </p>
            </statement>
            <hint>
                <p>
                    Use the fact that the determinant is linear on the columns of a matrix.
                </p>
            </hint>
        </exercise>
    </exercises>
</section>